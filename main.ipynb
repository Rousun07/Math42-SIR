{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Exploration - Text Track"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For illustration purposes and to establish the pipeline for initial exploration, this analysis uses a dataset containing a sample of 10,000 data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = \"yelp_sampled_data.csv\"\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data Processing\n",
    "\n",
    "  **To-Do:**\n",
    "\n",
    "  * The columns **\"address,\" \"city,\" \"state,\" \"postal_code,\" \"latitude,\"** and **\"longitude\"** contain geographical location data for restaurants or businesses. However, the dataset available at [Kaggle](https://www.kaggle.com/datasets/thedevastator/2013-irs-us-income-data-by-zip-code) provides average income information for each ZIP code as of 2013, which may be more useful. We can merge this dataset based on ZIP codes and remove the original location-related columns.\n",
    "\n",
    "  * The **\"BusinessParking\"** and **\"Ambience\"** columns contain nested attributes. If present, they store structured data in dictionary-like formats (e.g., for **BusinessParking**:  \n",
    "    `\"{'garage': False, 'street': False, 'validated': False, 'lot': False, 'valet': False}\"`,  \n",
    "    for **Ambience**:  \n",
    "    `\"{'touristy': False, 'hipster': False, 'romantic': False, 'divey': False, 'intimate': False, 'trendy': False, 'upscale': False, 'classy': False, 'casual': False}\"`).  \n",
    "    We need to extract and process these nested attributes, where one-hot encoding should be a suitable approach.\n",
    "\n",
    "  * Building on the previous point, if a business lacks **BusinessParking** or **Ambience** data, we need to determine an appropriate way to handle missing values in the one-hot encoding. Setting all values to zero might be a reasonable default.\n",
    "\n",
    "  * The **\"RestaurantsAttire\"** column contains inconsistent values, including:  \n",
    "    `\"u'casual'\"`, `nan`, `\"'casual'\"`, `\"u'dressy'\"`, `\"'dressy'\"`, `\"'formal'\"`, `\"u'formal'\"`.  \n",
    "    We need to standardize these values and clarify their meanings.\n",
    "\n",
    "  * The **\"categories\"** column stores a comma-separated string of labels, such as:  \n",
    "    `\"Restaurants, Gluten-Free, Bars, Food, Nightlife, Sandwiches, Burgers\"`.  \n",
    "    We should split these into individual categorical variables for better usability.\n",
    "\n",
    "- Text Processing\n",
    "\n",
    "  Below are some basic text processing methods considered:\n",
    "\n",
    "  - **Sentiment Analysis**\n",
    "\n",
    "  - **SBERT Embeddings**\n",
    "\n",
    "  - **Topic Modeling (LDA)**\n",
    "\n",
    "  - **TF-IDF Features**\n",
    "\n",
    "  These methods, while conventional, may introduce excessive noise, sparsity, or irrelevant features. Some approaches (e.g., LDA, TF-IDF) lack deep contextual understanding, while others (e.g., SBERT) may be too complex relative to the dataset size. \n",
    "  \n",
    "  **To-Do:**\n",
    "  \n",
    "  * Further Investigation of Existing Methods\n",
    "\n",
    "    * Evaluate whether they actually help with prediction. We can add one technique at a time to see if it helps with performance. Some other useful techniques include permutation importance or SHAP values for feature selection.\n",
    "    \n",
    "    * Implement other preprocessing steps for simple methods (TF-IDF and LDA), including stopword removal, stemming, etc.\n",
    "    \n",
    "    * Some minor things to do for specific technique, including hyperparameter tuning for LDA, comparison of sentiment scores against key business indicators to determine their predictive value. Also SBERT embeddings might not be the most optimal embedding, so we can try Word2Vec or GloVe as well. \n",
    "  \n",
    "  * Other NLP Techniques\n",
    "\n",
    "    * Experiment with domain-specific adaptations of BERT (e.g., DistilBERT, RoBERTa) using a small dataset of labeled business reviews.  \n",
    "\n",
    "    * Use transformer-based models to extract contextualized word importance instead of relying solely on TF-IDF.  \n",
    "\n",
    "    * Develop a pipeline combining LDA for topic extraction with embeddings for semantic similarity scoring.  \n",
    "\n",
    "    * Investigate if zero-shot classifiers like Mistral or OpenAI’s CLIP can generalize well on sentiment and topic classification without requiring extensive labeled data.  \n",
    "\n",
    "- Modeling\n",
    "\n",
    "  The current model is used primarily for testing purposes. A key challenge in this project is the severe class imbalance in target variables such as **usefulness** and **price range**, which requires careful handling.\n",
    "\n",
    "  **To-Do:**\n",
    "  - select an appropriate model, training strategy, and hyperparameter tuning approach.\n",
    "  - research and implement techniques to address class imbalance.\n",
    "  - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Attributes for Businesses\n",
    "\n",
    "There are two primary approaches for this dataset:  \n",
    "1. Predict attributes at the **individual review** level.  \n",
    "2. Predict attributes at the **business** level.  \n",
    "\n",
    "If we aim to predict business attributes such as **price range, ambience,** or similar characteristics, the following roadmap outlines the necessary steps.\n",
    "\n",
    "1. revert to the full dataset (instead of the sample dataset) and apply the following transformations.\n",
    "2. since the goal is to predict attributes for businesses rather than individual reviews, we should aggregate the dataset at the **business level**.\n",
    "3. decide how to handle various attributes. Below are suggested approaches:\n",
    "    - remove `user_id' since user identity is irrelevant for business-level predictions\n",
    "    - the following attributes should remain **constant** for a business across all its reviews. We can simply copy them from any review associated with that business:  \n",
    "        - `business_id`\n",
    "        - `name`\n",
    "        - `business_stars`\n",
    "        - `business_review_count`\n",
    "        - `BusinessParking`\n",
    "        - `Ambience`\n",
    "        - `RestaurantsAttire`\n",
    "        - `RestaurantsPriceRange2`\n",
    "        - `categories`\n",
    "    - `stars`, `useful`, `cool`: These are properties of individual reviews. Are they still useful when aggregating at the business level? We need to determine whether to retain, aggregate (e.g., average), or discard them.\n",
    "    - `user_review_count`, `user_useful`, `user_funny`, `user_cool`, `average_stars`, `fans`, `compliment_hot`, `compliment_more`, `compliment_profile`, `compliment_cute`, `compliment_list`, `compliment_note`, `compliment_plain`, `compliment_cool`, `compliment_funny`, `compliment_writer`, `compliment_photos`  \n",
    "    These attributes describe individual users. Should we **remove** them entirely, or **aggregate** them (e.g., taking the median) as a proxy for a typical reviewer’s profile for each business?\n",
    "    - `text`: we can combine all review texts for a business into a single large paragraph while maintaining structure to differentiate individual reviews. One possible format:  \"{review: ...} {review: ...}...\"\n",
    "4. once the transformations are complete, we should extract a subset of businesses from the full dataset for exploratory analysis.\n",
    "5. from this point onward, the workflow will align with the framework for predicting at the **individual review level** (outlined later). The main difference will be the **target variable**, but adjusting the framework should be straightforward.\n",
    "\n",
    "\n",
    "# Predicting Attributes for Individual Reviews\n",
    "\n",
    "The following code provides an example of how to predict an attribute for each individual review. In this case, we use \"useful\" votes as the attribute of interest.\n",
    "\n",
    "For each business, we first calculate the average number of useful votes received across all its reviews. Each individual review is then compared against this average and classified into one of three categories:\n",
    "\n",
    "- \"Average\" – if the review's useful votes are close to the business's average.\n",
    "\n",
    "- \"More useful\" – if the review has significantly more useful votes than the business's average.\n",
    "\n",
    "- \"Less useful\" – if the review has significantly fewer useful votes.\n",
    "\n",
    "This approach allows us to categorize reviews relative to their business, rather than using an absolute threshold, making the classification more context-aware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'business_id', 'stars', 'useful', 'cool', 'text', 'name',\n",
       "       'address', 'city', 'state', 'postal_code', 'latitude', 'longitude',\n",
       "       'business_stars', 'business_review_count', 'BusinessParking',\n",
       "       'Ambience', 'RestaurantsAttire', 'RestaurantsPriceRange2', 'categories',\n",
       "       'user_review_count', 'user_useful', 'user_funny', 'user_cool',\n",
       "       'average_stars', 'fans', 'compliment_hot', 'compliment_more',\n",
       "       'compliment_profile', 'compliment_cute', 'compliment_list',\n",
       "       'compliment_note', 'compliment_plain', 'compliment_cool',\n",
       "       'compliment_funny', 'compliment_writer', 'compliment_photos'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.decomposition import PCA, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.utils.class_weight import compute_sample_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   user_id             business_id  stars  useful  cool  \\\n",
      "1   bDjP8ELwG0OIWHsLgYyhLw  9vH3pJlBjfhi8HFlX9EB1w    5.0       1     0   \n",
      "2   NRHPcLq2vGWqgqwVugSgnQ  8sf9kv6O4GgEb0j1o22N1g    5.0       0     0   \n",
      "7   _cpU0VVdQcfN5AnuL6M56A  CADaY34LnEGjpSpU7Lee8w    4.0      31    24   \n",
      "13  mUINJT7vETh8ds9-jQhtiQ  ksisjnLbytLO8wlbxPZoCg    1.0       0     0   \n",
      "16  NeeSsIDvn-5-OYxMyp8doQ  R3FDYMQBrMpkUquwE-eniQ    3.0       0     0   \n",
      "\n",
      "                                                 text  \\\n",
      "1   I had recently started looking for a new hair ...   \n",
      "2   Jim Woltman who works at Goleta Honda is 5 sta...   \n",
      "7   Great lil bar on strip. Waitress was sweet she...   \n",
      "13  Decided to stop by and try this place again be...   \n",
      "16  Kind of a cool place... it's an old diner conv...   \n",
      "\n",
      "                                          name                 address  \\\n",
      "1                                     Mi Salon    1221 State St, Ste 4   \n",
      "2                          Santa Barbara Honda       475 S Kellogg Ave   \n",
      "7                            Surfside Taphouse        395 Mandalay Ave   \n",
      "13                                 Yummy Asian  479 S Oxford Valley Rd   \n",
      "16  The Continental Restaurant and Martini Bar           138 Market St   \n",
      "\n",
      "              city state  ... compliment_profile  compliment_cute  \\\n",
      "1    Santa Barbara    CA  ...                0.0              0.0   \n",
      "2           Goleta    CA  ...                0.0              0.0   \n",
      "7       Clearwater    FL  ...               57.0             21.0   \n",
      "13  Fairless Hills    PA  ...                0.0              0.0   \n",
      "16    Philadelphia    PA  ...                0.0              0.0   \n",
      "\n",
      "    compliment_list  compliment_note  compliment_plain compliment_cool  \\\n",
      "1               0.0              0.0               0.0             0.0   \n",
      "2               0.0              0.0               0.0             0.0   \n",
      "7               8.0           4144.0            9843.0         20141.0   \n",
      "13              0.0              0.0               0.0             0.0   \n",
      "16              0.0              0.0               0.0             0.0   \n",
      "\n",
      "   compliment_funny compliment_writer  compliment_photos useful_category  \n",
      "1               0.0               0.0                0.0     more useful  \n",
      "2               0.0               0.0                0.0     less useful  \n",
      "7           20141.0             335.0            18401.0     more useful  \n",
      "13              0.0               0.0                0.0     less useful  \n",
      "16              0.0               0.0                0.0     less useful  \n",
      "\n",
      "[5 rows x 38 columns]\n"
     ]
    }
   ],
   "source": [
    "# 1. Drop missing rows for 'useful' or 'business_id'\n",
    "df_filtered = df.dropna(subset=['useful', 'business_id'])\n",
    "\n",
    "# 2. Compute 40th and 60th percentile of \"useful\" per business\n",
    "#    transform() ensures the percentile values are repeated for each row in that group\n",
    "df_filtered['p40'] = df_filtered.groupby('business_id')['useful']\\\n",
    "                                .transform(lambda x: x.quantile(0.40))\n",
    "df_filtered['p60'] = df_filtered.groupby('business_id')['useful']\\\n",
    "                                .transform(lambda x: x.quantile(0.60))\n",
    "\n",
    "# 3. Define classification function based on 40th/60th cutoffs\n",
    "def classify_useful(row):\n",
    "    if row['useful'] < row['p40']:\n",
    "        return \"less useful\"\n",
    "    elif row['useful'] > row['p60']:\n",
    "        return \"more useful\"\n",
    "    else:\n",
    "        return \"average\"\n",
    "\n",
    "df_filtered['useful_category'] = df_filtered.apply(classify_useful, axis=1)\n",
    "\n",
    "# 4. Optionally remove the \"average\" rows to focus on a 2-class problem\n",
    "df_filtered = df_filtered[df_filtered['useful_category'] != 'average']\n",
    "\n",
    "# 5. Drop the temporary percentile columns if you don’t need them later\n",
    "df_filtered.drop(['p40', 'p60'], axis=1, inplace=True)\n",
    "\n",
    "# Now df_filtered has only two categories: 'more useful' and 'less useful'\n",
    "print(df_filtered.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert useful votes to useful classifications\n",
    "\n",
    "df_filtered = df.dropna(subset = ['useful', 'business_id'])\n",
    "\n",
    "\n",
    "# preprocessing for useful categories\n",
    "business_avg_useful = df_filtered.groupby('business_id')['useful'].mean().reset_index()\n",
    "business_avg_useful.rename(columns = {'useful': 'average_business_useful'}, inplace = True)\n",
    "\n",
    "# merge the average back\n",
    "df_filtered = df_filtered.merge(business_avg_useful, on = 'business_id', how = 'left')\n",
    "\n",
    "def classify_useful(row):\n",
    "    if row['useful'] > row['average_business_useful']:\n",
    "        return \"more useful\"\n",
    "    elif row['useful'] < row['average_business_useful']:\n",
    "        return \"less useful\"\n",
    "    else:\n",
    "        return \"average\"\n",
    "\n",
    "df_filtered['useful_category'] = df_filtered.apply(classify_useful, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_filtered['useful_category']\n",
    "#X = df_filtered.drop(columns=['useful', 'average_business_useful', 'useful_category'])\n",
    "X = df_filtered.drop(columns=['useful', 'useful_category'])\n",
    "\n",
    "categorical_cols = ['state', 'city', 'categories', 'BusinessParking', 'Ambience', 'RestaurantsAttire']\n",
    "numerical_cols = ['stars', 'RestaurantsPriceRange2', 'cool', 'business_stars', 'business_review_count',\n",
    "                  'user_review_count', 'user_useful', 'user_funny', 'user_cool', 'average_stars', 'fans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment analysis\n",
    "df_filtered['sentiment'] = df_filtered['text'].apply(lambda x: TextBlob(str(x)).sentiment.polarity)\n",
    "numerical_cols.append('sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SBERT embeddings\n",
    "\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "text_embeddings_sbert = sbert_model.encode(df_filtered['text'].tolist(), convert_to_numpy = True)\n",
    "pca_sbert = PCA(n_components = 50, random_state = 42)\n",
    "text_embeddings_sbert_reduced = pca_sbert.fit_transform(text_embeddings_sbert)\n",
    "embedding_cols_sbert = [f\"text_sbert_pca_{i}\" for i in range(50)]\n",
    "df_embeddings_sbert = pd.DataFrame(text_embeddings_sbert_reduced, columns = embedding_cols_sbert, index = df_filtered.index)\n",
    "df_filtered = pd.concat([df_filtered, df_embeddings_sbert], axis=1)\n",
    "numerical_cols.extend(embedding_cols_sbert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA topic modeling\n",
    "\n",
    "vectorizer = CountVectorizer(max_features = 1000, stop_words = 'english')\n",
    "X_text = vectorizer.fit_transform(df_filtered['text'])\n",
    "lda = LatentDirichletAllocation(n_components = 5, random_state = 42)\n",
    "lda_topics = lda.fit_transform(X_text)\n",
    "topic_cols = [f\"topic_{i}\" for i in range(5)]\n",
    "df_topics = pd.DataFrame(lda_topics, columns = topic_cols, index = df_filtered.index)\n",
    "df_filtered = pd.concat([df_filtered, df_topics], axis=1)\n",
    "numerical_cols.extend(topic_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features = 500, stop_words = 'english')\n",
    "X_tfidf = tfidf.fit_transform(df_filtered['text'])\n",
    "tfidf_cols = [f\"tfidf_{word}\" for word in tfidf.get_feature_names_out()]\n",
    "df_tfidf = pd.DataFrame(X_tfidf.toarray(), columns = tfidf_cols, index = df_filtered.index)\n",
    "df_filtered = pd.concat([df_filtered, df_tfidf], axis = 1)\n",
    "numerical_cols.extend(tfidf_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_filtered.drop(columns = ['text'])\n",
    "\n",
    "# preprocessing pipelines\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy = 'mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy = 'most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown = 'ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numerical_pipeline, numerical_cols),\n",
    "    ('cat', categorical_pipeline, categorical_cols)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try new training: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encoded = preprocessor.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 500],  # Number of trees in the forest\n",
    "    'max_depth': [10, 20, 30, None],       # Depth of each tree\n",
    "    'min_samples_split': [2, 5, 10],       # Minimum samples needed to split a node\n",
    "    'min_samples_leaf': [1, 2, 4],         # Minimum samples per leaf\n",
    "    'max_features': ['sqrt', 'log2'],      # Number of features considered for splits\n",
    "    'bootstrap': [True, False]             # Bootstrap sampling for trees\n",
    "}\n",
    "\n",
    "# Initialize Random Forest with class weights\n",
    "rf_classifier = RandomForestClassifier(random_state=42, class_weight={'average': 1, 'less useful': 20, 'more useful': 20})\n",
    "\n",
    "# Perform hyperparameter tuning using RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf_classifier,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=15,  # Number of random combinations to try\n",
    "    cv=3,       # 3-fold cross-validation\n",
    "    scoring='f1_weighted',  # Optimize for F1 score due to class imbalance\n",
    "    random_state=42,\n",
    "    n_jobs=-1   # Use all available processors\n",
    ")\n",
    "\n",
    "# Fit the model with SMOTE-resampled training data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best classifier from tuning\n",
    "best_classifier = random_search.best_estimator_\n",
    "\n",
    "# Train the best classifier\n",
    "best_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = best_classifier.predict(X_test)\n",
    "\n",
    "# Display the best parameters\n",
    "print(\"Best Parameters Found:\", random_search.best_params_)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data and split into train/test sets\n",
    "X_encoded = preprocessor.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size = 0.2, random_state = 42, stratify = y)\n",
    "\n",
    "# Apply SMOTE to handle class imbalance\n",
    "smote = SMOTE(random_state = 42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "classifier = RandomForestClassifier(n_estimators = 200, random_state = 42, class_weight = \n",
    "                                    {'average': 1, 'less useful': 20, 'more useful': 20})\n",
    "classifier.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"\\nAccuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification visualization\n",
    "\n",
    "class_labels = sorted(y.unique())\n",
    "conf_matrix = confusion_matrix(y_test, y_pred, labels=class_labels)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "ax = sns.heatmap(conf_matrix, annot = False, cmap = 'Blues', xticklabels = class_labels, yticklabels = class_labels)\n",
    "min_val, max_val = conf_matrix.min(), conf_matrix.max()\n",
    "threshold = (min_val + max_val) / 2\n",
    "\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        value = conf_matrix[i, j]\n",
    "        text_color = \"white\" if value > threshold else \"black\"  # Choose color based on intensity\n",
    "        ax.text(j + 0.5, i + 0.5, value, ha = 'center', va = 'center', color = text_color, fontsize = 12)\n",
    "\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
